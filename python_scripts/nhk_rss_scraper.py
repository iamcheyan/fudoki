#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NHK RSSé“¾æ¥æŠ“å–å™¨
æŠ“å–NHKå®˜ç½‘çš„RSSé“¾æ¥å¹¶æŒ‰åˆ†ç±»æ‰“å°
"""

import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import json
from typing import Dict, List, Tuple

class NHKRSSScraper:
    def __init__(self):
        self.base_url = "https://www.nhk.or.jp"
        self.rss_page_url = "https://www.nhk.or.jp/toppage/rss/index.html"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def fetch_page(self) -> str:
        """è·å–NHK RSSé¡µé¢å†…å®¹"""
        try:
            response = self.session.get(self.rss_page_url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            print(f"è·å–é¡µé¢å¤±è´¥: {e}")
            return ""
    
    def parse_rss_links(self, html_content: str) -> Dict[str, List[str]]:
        """è§£æRSSé“¾æ¥å¹¶æŒ‰åˆ†ç±»æ•´ç†"""
        soup = BeautifulSoup(html_content, 'html.parser')
        rss_links = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        links = soup.find_all('a', href=True)
        
        # RSSé“¾æ¥æ¨¡å¼
        rss_patterns = [
            r'\.rss$',
            r'\.xml$',
            r'/rss/',
            r'/feed/'
        ]
        
        # åˆ†ç±»æ˜ å°„
        category_mapping = {
            'ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹': ['ä¸»è¦', 'ãƒ¡ã‚¤ãƒ³', 'top', 'main'],
            'ç¤¾ä¼š': ['ç¤¾ä¼š', 'society', 'social'],
            'æ–‡åŒ–ãƒ»ã‚¨ãƒ³ã‚¿ãƒ¡': ['æ–‡åŒ–', 'ã‚¨ãƒ³ã‚¿ãƒ¡', 'culture', 'entertainment', 'entertain'],
            'ç§‘å­¦ãƒ»åŒ»ç™‚': ['ç§‘å­¦', 'åŒ»ç™‚', 'science', 'medical', 'health'],
            'æ”¿æ²»': ['æ”¿æ²»', 'politics', 'political'],
            'çµŒæ¸ˆ': ['çµŒæ¸ˆ', 'economy', 'economic', 'business'],
            'å›½éš›': ['å›½éš›', 'international', 'world'],
            'ã‚¹ãƒãƒ¼ãƒ„': ['ã‚¹ãƒãƒ¼ãƒ„', 'sports', 'sport']
        }
        
        for link in links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯RSSé“¾æ¥
            is_rss = any(re.search(pattern, href, re.IGNORECASE) for pattern in rss_patterns)
            
            if is_rss:
                # æ„å»ºå®Œæ•´URL
                full_url = urljoin(self.base_url, href)
                
                # æ ¹æ®é“¾æ¥æ–‡æœ¬æˆ–URLç¡®å®šåˆ†ç±»
                category = self._categorize_link(text, href, category_mapping)
                
                if category not in rss_links:
                    rss_links[category] = []
                
                rss_links[category].append({
                    'title': text,
                    'url': full_url,
                    'original_href': href
                })
        
        return rss_links
    
    def _categorize_link(self, text: str, href: str, category_mapping: Dict) -> str:
        """æ ¹æ®é“¾æ¥æ–‡æœ¬å’ŒURLç¡®å®šåˆ†ç±»"""
        text_lower = text.lower()
        href_lower = href.lower()
        
        for category, keywords in category_mapping.items():
            for keyword in keywords:
                if keyword.lower() in text_lower or keyword.lower() in href_lower:
                    return category
        
        return "ãã®ä»–"
    
    def get_rss_feeds_from_content(self, html_content: str) -> Dict[str, List[str]]:
        """ä»é¡µé¢å†…å®¹ä¸­æå–RSSé“¾æ¥ï¼ˆåŸºäºé¡µé¢æ–‡æœ¬åˆ†æï¼‰"""
        # æ ¹æ®ç½‘é¡µå†…å®¹ï¼ŒNHKæä¾›äº†ä»¥ä¸‹RSSåˆ†ç±»
        rss_categories = {
            'NHKä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ ç¤¾ä¼š': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ æ–‡åŒ–ãƒ»ã‚¨ãƒ³ã‚¿ãƒ¡': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ ç§‘å­¦ãƒ»åŒ»ç™‚': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ æ”¿æ²»': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ çµŒæ¸ˆ': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ å›½éš›': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ãƒãƒ¼ãƒ„': []
        }
        
        # å°è¯•ä»é¡µé¢ä¸­æŸ¥æ‰¾å®é™…çš„RSSé“¾æ¥
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„RSSé“¾æ¥
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯RSSç›¸å…³é“¾æ¥
            if any(keyword in href.lower() for keyword in ['rss', 'feed', '.xml']):
                full_url = urljoin(self.base_url, href)
                
                # æ ¹æ®æ–‡æœ¬å†…å®¹åˆ†ç±»
                for category in rss_categories.keys():
                    if any(keyword in text for keyword in ['ä¸»è¦', 'ç¤¾ä¼š', 'æ–‡åŒ–', 'ç§‘å­¦', 'æ”¿æ²»', 'çµŒæ¸ˆ', 'å›½éš›', 'ã‚¹ãƒãƒ¼ãƒ„']):
                        rss_categories[category].append(full_url)
                        break
        
        return rss_categories
    
    def scrape_and_print(self):
        """ä¸»è¦æ–¹æ³•ï¼šæŠ“å–å¹¶æ‰“å°RSSé“¾æ¥"""
        print("=" * 60)
        print("NHK RSSé“¾æ¥æŠ“å–å™¨")
        print("=" * 60)
        
        # è·å–é¡µé¢å†…å®¹
        print("æ­£åœ¨è·å–NHK RSSé¡µé¢...")
        html_content = self.fetch_page()
        
        if not html_content:
            print("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
            return
        
        print("âœ… é¡µé¢è·å–æˆåŠŸ")
        print()
        
        # è§£æRSSé“¾æ¥
        print("æ­£åœ¨è§£æRSSé“¾æ¥...")
        rss_links = self.parse_rss_links(html_content)
        
        if not rss_links:
            print("âš ï¸  æœªæ‰¾åˆ°RSSé“¾æ¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
            rss_links = self.get_rss_feeds_from_content(html_content)
        
        # æ‰“å°ç»“æœ
        if rss_links:
            print("âœ… æ‰¾åˆ°ä»¥ä¸‹RSSé“¾æ¥:")
            print()
            
            total_links = 0
            for category, links in rss_links.items():
                if links:
                    print(f"ğŸ“‚ {category}:")
                    for i, link_info in enumerate(links, 1):
                        if isinstance(link_info, dict):
                            print(f"  {i}. {link_info['title']}")
                            print(f"     URL: {link_info['url']}")
                        else:
                            print(f"  {i}. {link_info}")
                        total_links += 1
                    print()
            
            print(f"ğŸ“Š æ€»è®¡æ‰¾åˆ° {total_links} ä¸ªRSSé“¾æ¥")
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•RSSé“¾æ¥")
            print("\nå¯èƒ½çš„åŸå› :")
            print("1. é¡µé¢ç»“æ„å‘ç”Ÿå˜åŒ–")
            print("2. RSSé“¾æ¥éœ€è¦ç‰¹æ®Šæƒé™è®¿é—®")
            print("3. éœ€è¦JavaScriptæ¸²æŸ“çš„å†…å®¹")
    
    def save_to_json(self, filename: str = "nhk_rss_links.json"):
        """å°†RSSé“¾æ¥ä¿å­˜åˆ°JSONæ–‡ä»¶"""
        html_content = self.fetch_page()
        if not html_content:
            return
        
        rss_links = self.parse_rss_links(html_content)
        if not rss_links:
            rss_links = self.get_rss_feeds_from_content(html_content)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(rss_links, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… RSSé“¾æ¥å·²ä¿å­˜åˆ° {filename}")

def main():
    """ä¸»å‡½æ•°"""
    scraper = NHKRSSScraper()
    
    # æŠ“å–å¹¶æ‰“å°RSSé“¾æ¥
    scraper.scrape_and_print()
    
    # å¯é€‰ï¼šä¿å­˜åˆ°JSONæ–‡ä»¶
    # scraper.save_to_json()

if __name__ == "__main__":
    main()
