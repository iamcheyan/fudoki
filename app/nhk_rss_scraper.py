#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NHK RSSæ–‡ç« æŠ“å–å™¨
æŠ“å–NHKå®˜ç½‘çš„RSSæ–‡ç« æ ‡é¢˜å’Œé“¾æ¥ï¼Œå¹¶æä¾›APIæ¥å£
"""

import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import json
from typing import Dict, List, Tuple
import xml.etree.ElementTree as ET
import os
from flask import Flask, jsonify, request
from flask_cors import CORS

class NHKRSSScraper:
    def __init__(self):
        self.base_url = "https://www.nhk.or.jp"
        self.rss_page_url = "https://www.nhk.or.jp/toppage/rss/index.html"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def fetch_page(self) -> str:
        """è·å–NHK RSSé¡µé¢å†…å®¹"""
        try:
            response = self.session.get(self.rss_page_url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            print(f"è·å–é¡µé¢å¤±è´¥: {e}")
            return ""
    
    def parse_rss_links(self, html_content: str) -> Dict[str, List[str]]:
        """è§£æRSSé“¾æ¥å¹¶æŒ‰åˆ†ç±»æ•´ç†"""
        soup = BeautifulSoup(html_content, 'html.parser')
        rss_links = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        links = soup.find_all('a', href=True)
        
        # RSSé“¾æ¥æ¨¡å¼
        rss_patterns = [
            r'\.rss$',
            r'\.xml$',
            r'/rss/',
            r'/feed/'
        ]
        
        # åˆ†ç±»æ˜ å°„
        category_mapping = {
            'ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹': ['ä¸»è¦', 'ãƒ¡ã‚¤ãƒ³', 'top', 'main'],
            'ç¤¾ä¼š': ['ç¤¾ä¼š', 'society', 'social'],
            'æ–‡åŒ–ãƒ»ã‚¨ãƒ³ã‚¿ãƒ¡': ['æ–‡åŒ–', 'ã‚¨ãƒ³ã‚¿ãƒ¡', 'culture', 'entertainment', 'entertain'],
            'ç§‘å­¦ãƒ»åŒ»ç™‚': ['ç§‘å­¦', 'åŒ»ç™‚', 'science', 'medical', 'health'],
            'æ”¿æ²»': ['æ”¿æ²»', 'politics', 'political'],
            'çµŒæ¸ˆ': ['çµŒæ¸ˆ', 'economy', 'economic', 'business'],
            'å›½éš›': ['å›½éš›', 'international', 'world'],
            'ã‚¹ãƒãƒ¼ãƒ„': ['ã‚¹ãƒãƒ¼ãƒ„', 'sports', 'sport']
        }
        
        for link in links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯RSSé“¾æ¥
            is_rss = any(re.search(pattern, href, re.IGNORECASE) for pattern in rss_patterns)
            
            if is_rss:
                # æ„å»ºå®Œæ•´URL
                full_url = urljoin(self.base_url, href)
                
                # æ ¹æ®é“¾æ¥æ–‡æœ¬æˆ–URLç¡®å®šåˆ†ç±»
                category = self._categorize_link(text, href, category_mapping)
                
                if category not in rss_links:
                    rss_links[category] = []
                
                rss_links[category].append({
                    'title': text,
                    'url': full_url,
                    'original_href': href
                })
        
        return rss_links
    
    def _categorize_link(self, text: str, href: str, category_mapping: Dict) -> str:
        """æ ¹æ®é“¾æ¥æ–‡æœ¬å’ŒURLç¡®å®šåˆ†ç±»"""
        text_lower = text.lower()
        href_lower = href.lower()
        
        for category, keywords in category_mapping.items():
            for keyword in keywords:
                if keyword.lower() in text_lower or keyword.lower() in href_lower:
                    return category
        
        return "ãã®ä»–"
    
    def get_rss_feeds_from_content(self, html_content: str) -> Dict[str, List[str]]:
        """ä»é¡µé¢å†…å®¹ä¸­æå–RSSé“¾æ¥ï¼ˆåŸºäºé¡µé¢æ–‡æœ¬åˆ†æï¼‰"""
        # æ ¹æ®ç½‘é¡µå†…å®¹ï¼ŒNHKæä¾›äº†ä»¥ä¸‹RSSåˆ†ç±»
        rss_categories = {
            'NHKä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ ç¤¾ä¼š': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ æ–‡åŒ–ãƒ»ã‚¨ãƒ³ã‚¿ãƒ¡': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ ç§‘å­¦ãƒ»åŒ»ç™‚': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ æ”¿æ²»': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ çµŒæ¸ˆ': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ å›½éš›': [],
            'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ãƒãƒ¼ãƒ„': []
        }
        
        # å°è¯•ä»é¡µé¢ä¸­æŸ¥æ‰¾å®é™…çš„RSSé“¾æ¥
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„RSSé“¾æ¥
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯RSSç›¸å…³é“¾æ¥
            if any(keyword in href.lower() for keyword in ['rss', 'feed', '.xml']):
                full_url = urljoin(self.base_url, href)
                
                # æ ¹æ®æ–‡æœ¬å†…å®¹åˆ†ç±»
                for category in rss_categories.keys():
                    if any(keyword in text for keyword in ['ä¸»è¦', 'ç¤¾ä¼š', 'æ–‡åŒ–', 'ç§‘å­¦', 'æ”¿æ²»', 'çµŒæ¸ˆ', 'å›½éš›', 'ã‚¹ãƒãƒ¼ãƒ„']):
                        rss_categories[category].append(full_url)
                        break
        
        return rss_categories
    
    def fetch_rss_articles(self, rss_url: str) -> List[Dict]:
        """è·å–RSS feedä¸­çš„æ–‡ç« åˆ—è¡¨"""
        try:
            response = self.session.get(rss_url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # è§£æXMLå†…å®¹
            root = ET.fromstring(response.content)
            articles = []
            
            # å¤„ç†RSS 2.0æ ¼å¼
            for item in root.findall('.//item'):
                title = item.find('title')
                link = item.find('link')
                description = item.find('description')
                pub_date = item.find('pubDate')
                
                if title is not None and link is not None:
                    article = {
                        'title': title.text.strip() if title.text else '',
                        'link': link.text.strip() if link.text else '',
                        'description': description.text.strip() if description is not None and description.text else '',
                        'pub_date': pub_date.text.strip() if pub_date is not None and pub_date.text else ''
                    }
                    articles.append(article)
            
            return articles
            
        except Exception as e:
            print(f"è·å–RSSæ–‡ç« å¤±è´¥ {rss_url}: {e}")
            return []
    
    def fetch_article_content(self, article_url: str) -> str:
        """è·å–æ–‡ç« å®Œæ•´å†…å®¹"""
        try:
            response = self.session.get(article_url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å°è¯•å¤šç§æ–¹æ³•è·å–æ–‡ç« å†…å®¹
            content_text = ""
            
            # æ–¹æ³•1: å°è¯•è·å–é¡µé¢æ ‡é¢˜
            title = soup.find('title')
            if title:
                content_text += title.get_text(strip=True) + "\n\n"
            
            # æ–¹æ³•2: å°è¯•è·å–metaæè¿°
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                content_text += meta_desc.get('content') + "\n\n"
            
            # æ–¹æ³•3: å°è¯•è·å–ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content_selectors = [
                '.content--detail-body',
                '.content--detail',
                '.article-body',
                '.article-content',
                '.content-body',
                'article',
                '.main-content',
                '.news-content'
            ]
            
            main_content = None
            for selector in main_content_selectors:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    if len(text) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                        main_content = element
                        break
            
            if main_content:
                # æ¸…ç†ä¸»è¦å†…å®¹
                for element in main_content(["script", "style", "nav", "header", "footer", "aside", "form"]):
                    element.decompose()
                
                # ç§»é™¤å¹¿å‘Šå’Œæ— å…³å…ƒç´ 
                for element in main_content.find_all(['div', 'section'], class_=lambda x: x and any(
                    keyword in x.lower() for keyword in ['ad', 'advertisement', 'sidebar', 'related', 'social', 'menu', 'navigation']
                )):
                    element.decompose()
                
                main_text = main_content.get_text(separator='\n', strip=True)
                content_text += main_text + "\n\n"
            
            # æ–¹æ³•4: å¦‚æœä¸»è¦å†…å®¹ä¸å¤Ÿï¼Œå°è¯•è·å–æ‰€æœ‰æ®µè½
            if len(content_text) < 200:
                paragraphs = soup.find_all('p')
                valid_paragraphs = []
                
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    # è¿‡æ»¤æ¡ä»¶ï¼šé•¿åº¦è¶³å¤Ÿï¼Œä¸æ˜¯å¯¼èˆª/å¹¿å‘Šå†…å®¹ï¼Œä¸æ˜¯é“¾æ¥
                    if (len(text) > 30 and 
                        not any(keyword in text.lower() for keyword in [
                            'copyright', 'nhk', 'æ”¾é€', 'å—ä¿¡', 'å¥‘ç´„', 'ãƒªãƒ³ã‚¯', 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰',
                            'æ”¾é€ç•ªçµ„', 'åŒæ™‚é…ä¿¡', 'è¦‹é€ƒã—é…ä¿¡', 'ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹', 'ç•ªçµ„é–¢é€£æƒ…å ±',
                            'äº‹æ¥­ã‚„å­¦æ ¡', 'ã”åˆ©ç”¨', 'ä¸‹è¨˜ã®ãƒªãƒ³ã‚¯', 'ç¢ºèªã—ã¦ãã ã•ã„'
                        ]) and
                        not text.startswith('http') and
                        not text.startswith('www') and
                        not text.startswith('10æœˆ') and  # è¿‡æ»¤æ—¥æœŸ
                        not text.startswith('9æœˆ') and
                        not text.startswith('å¤§é›¨æƒ…å ±') and
                        not text.startswith('ã‚¦ã‚©ãƒ¼ãƒ«è¡—') and
                        not text.startswith('ã‚³ãƒ¡å¢—ç”£') and
                        not text.startswith('ã€Œã‚ã®æ™‚') and
                        not text.startswith('iPSç´°èƒ') and
                        not text.startswith('40å¹´å‰') and
                        not text.startswith('è‡ªæ°‘å…š') and
                        not text.startswith('å›½é€£ãŒ') and
                        not text.startswith('ä¸–ç•Œåˆ')):
                        valid_paragraphs.append(text)
                
                if valid_paragraphs:
                    content_text += "\n".join(valid_paragraphs)
            
            # æ¸…ç†å’Œæ ¼å¼åŒ–å†…å®¹
            if content_text:
                # ç§»é™¤é‡å¤è¡Œ
                lines = []
                seen_lines = set()
                for line in content_text.split('\n'):
                    line = line.strip()
                    if line and line not in seen_lines:
                        lines.append(line)
                        seen_lines.add(line)
                
                cleaned_content = '\n'.join(lines)
                
                # å¦‚æœå†…å®¹ä»ç„¶å¤ªçŸ­ï¼Œè¿”å›åŸå§‹RSSæè¿°
                if len(cleaned_content) < 100:
                    return "æ–‡ç« å†…å®¹è·å–å¤±è´¥ï¼Œè¯·ç‚¹å‡»'æŸ¥çœ‹åŸæ–‡'æŒ‰é’®è®¿é—®åŸå§‹é¡µé¢"
                
                return cleaned_content[:5000]  # å¢åŠ é•¿åº¦é™åˆ¶
            else:
                return "æ— æ³•è·å–æ–‡ç« å†…å®¹"
                
        except Exception as e:
            print(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥ {article_url}: {e}")
            return f"è·å–æ–‡ç« å†…å®¹å¤±è´¥: {str(e)}"
    
    def get_all_articles(self) -> Dict[str, List[Dict]]:
        """è·å–æ‰€æœ‰åˆ†ç±»çš„NHKæ–‡ç« """
        print("æ­£åœ¨è·å–NHKæ–‡ç« ...")
        
        # å·²çŸ¥çš„NHK RSSé“¾æ¥
        rss_feeds = {
            'ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹': 'https://www3.nhk.or.jp/rss/news/cat0.xml',
            'ç¤¾ä¼š': 'https://www3.nhk.or.jp/rss/news/cat1.xml',
            'æ–‡åŒ–ãƒ»ã‚¨ãƒ³ã‚¿ãƒ¡': 'https://www3.nhk.or.jp/rss/news/cat2.xml',
            'ç§‘å­¦ãƒ»åŒ»ç™‚': 'https://www3.nhk.or.jp/rss/news/cat3.xml',
            'æ”¿æ²»': 'https://www3.nhk.or.jp/rss/news/cat4.xml',
            'çµŒæ¸ˆ': 'https://www3.nhk.or.jp/rss/news/cat5.xml',
            'å›½éš›': 'https://www3.nhk.or.jp/rss/news/cat6.xml',
            'ã‚¹ãƒãƒ¼ãƒ„': 'https://www3.nhk.or.jp/rss/news/cat7.xml'
        }
        
        all_articles = {}
        
        for category, rss_url in rss_feeds.items():
            print(f"æ­£åœ¨è·å– {category} æ–‡ç« ...")
            articles = self.fetch_rss_articles(rss_url)
            if articles:
                all_articles[category] = articles
                print(f"âœ… {category}: æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            else:
                print(f"âŒ {category}: æœªæ‰¾åˆ°æ–‡ç« ")
        
        return all_articles
    
    def scrape_and_print(self):
        """ä¸»è¦æ–¹æ³•ï¼šæŠ“å–å¹¶æ‰“å°RSSé“¾æ¥"""
        print("=" * 60)
        print("NHK RSSé“¾æ¥æŠ“å–å™¨")
        print("=" * 60)
        
        # è·å–é¡µé¢å†…å®¹
        print("æ­£åœ¨è·å–NHK RSSé¡µé¢...")
        html_content = self.fetch_page()
        
        if not html_content:
            print("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
            return
        
        print("âœ… é¡µé¢è·å–æˆåŠŸ")
        print()
        
        # è§£æRSSé“¾æ¥
        print("æ­£åœ¨è§£æRSSé“¾æ¥...")
        rss_links = self.parse_rss_links(html_content)
        
        if not rss_links:
            print("âš ï¸  æœªæ‰¾åˆ°RSSé“¾æ¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
            rss_links = self.get_rss_feeds_from_content(html_content)
        
        # æ‰“å°ç»“æœ
        if rss_links:
            print("âœ… æ‰¾åˆ°ä»¥ä¸‹RSSé“¾æ¥:")
            print()
            
            total_links = 0
            for category, links in rss_links.items():
                if links:
                    print(f"ğŸ“‚ {category}:")
                    for i, link_info in enumerate(links, 1):
                        if isinstance(link_info, dict):
                            print(f"  {i}. {link_info['title']}")
                            print(f"     URL: {link_info['url']}")
                        else:
                            print(f"  {i}. {link_info}")
                        total_links += 1
                    print()
            
            print(f"ğŸ“Š æ€»è®¡æ‰¾åˆ° {total_links} ä¸ªRSSé“¾æ¥")
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•RSSé“¾æ¥")
            print("\nå¯èƒ½çš„åŸå› :")
            print("1. é¡µé¢ç»“æ„å‘ç”Ÿå˜åŒ–")
            print("2. RSSé“¾æ¥éœ€è¦ç‰¹æ®Šæƒé™è®¿é—®")
            print("3. éœ€è¦JavaScriptæ¸²æŸ“çš„å†…å®¹")
    
    def save_to_json(self, filename: str = "nhk_rss_links.json"):
        """å°†RSSé“¾æ¥ä¿å­˜åˆ°JSONæ–‡ä»¶"""
        html_content = self.fetch_page()
        if not html_content:
            return
        
        rss_links = self.parse_rss_links(html_content)
        if not rss_links:
            rss_links = self.get_rss_feeds_from_content(html_content)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(rss_links, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… RSSé“¾æ¥å·²ä¿å­˜åˆ° {filename}")

# Flask APIåº”ç”¨
app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€scraperå®ä¾‹
scraper = NHKRSSScraper()

@app.route('/api/nhk/articles', methods=['GET'])
def get_nhk_articles():
    """è·å–NHKæ–‡ç« APIæ¥å£"""
    try:
        articles = scraper.get_all_articles()
        return jsonify({
            'success': True,
            'data': articles,
            'total_categories': len(articles),
            'total_articles': sum(len(articles_list) for articles_list in articles.values())
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/nhk/articles/<category>', methods=['GET'])
def get_nhk_articles_by_category(category):
    """è·å–ç‰¹å®šåˆ†ç±»çš„NHKæ–‡ç« """
    try:
        # åˆ†ç±»æ˜ å°„
        category_mapping = {
            'main': 'ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹',
            'society': 'ç¤¾ä¼š',
            'culture': 'æ–‡åŒ–ãƒ»ã‚¨ãƒ³ã‚¿ãƒ¡',
            'science': 'ç§‘å­¦ãƒ»åŒ»ç™‚',
            'politics': 'æ”¿æ²»',
            'economy': 'çµŒæ¸ˆ',
            'international': 'å›½éš›',
            'sports': 'ã‚¹ãƒãƒ¼ãƒ„'
        }
        
        japanese_category = category_mapping.get(category, category)
        articles = scraper.get_all_articles()
        
        if japanese_category in articles:
            return jsonify({
                'success': True,
                'data': articles[japanese_category],
                'category': japanese_category,
                'count': len(articles[japanese_category])
            })
        else:
            return jsonify({
                'success': False,
                'error': f'åˆ†ç±» {japanese_category} ä¸å­˜åœ¨'
            }), 404
            
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/nhk/article-content', methods=['GET'])
def get_article_content():
    """è·å–æ–‡ç« å®Œæ•´å†…å®¹"""
    try:
        article_url = request.args.get('url')
        if not article_url:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘æ–‡ç« URLå‚æ•°'
            }), 400
        
        scraper = NHKRSSScraper()
        content = scraper.fetch_article_content(article_url)
        
        return jsonify({
            'success': True,
            'content': content,
            'url': article_url
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/nhk/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'success': True,
        'message': 'NHK RSS API is running',
        'status': 'healthy'
    })

def main():
    """ä¸»å‡½æ•°"""
    scraper = NHKRSSScraper()
    
    # æŠ“å–å¹¶æ‰“å°RSSé“¾æ¥
    scraper.scrape_and_print()
    
    # å¯é€‰ï¼šä¿å­˜åˆ°JSONæ–‡ä»¶
    # scraper.save_to_json()

def run_api(host='127.0.0.1', port=None, debug=False):
    """è¿è¡ŒAPIæœåŠ¡å™¨"""
    # ä»ç¯å¢ƒå˜é‡è·å–ç«¯å£ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    if port is None:
        port = int(os.environ.get('FLASK_PORT', 5000))
    
    print(f"å¯åŠ¨NHK RSS APIæœåŠ¡å™¨...")
    print(f"APIåœ°å€: http://{host}:{port}")
    print(f"æ–‡ç« æ¥å£: http://{host}:{port}/api/nhk/articles")
    print(f"å¥åº·æ£€æŸ¥: http://{host}:{port}/api/nhk/health")
    app.run(host=host, port=port, debug=debug)

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == 'api':
        run_api()
    else:
        main()
